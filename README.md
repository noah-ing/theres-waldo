# ğŸ” There's Waldo: Modern Vision Transformer for Finding Waldo

A state-of-the-art implementation for solving "Where's Waldo?" puzzles using JAX and Vision Transformers. This project demonstrates modern deep learning practices and efficient implementation using JAX's automatic differentiation and compilation capabilities.

![Waldo Detection Example](docs/docs.png)

## ğŸš€ Key Features

- **Modern Architecture**: Vision Transformer (ViT) backbone with DETR-style detection head
- **High Performance**: JAX-based implementation with JIT compilation and automatic differentiation
- **Advanced Training**: 
  - Focal Loss for handling class imbalance
  - GIoU Loss for better bounding box regression
  - Automatic Mixed Precision (AMP) training
  - Gradient clipping and learning rate scheduling
- **Rich Augmentations**: Comprehensive data augmentation pipeline including:
  - Random flipping
  - Color jittering
  - Brightness/contrast adjustments
  - Aspect ratio preservation
- **MLOps Best Practices**:
  - Experiment tracking with Weights & Biases
  - Configuration management with Hydra
  - Type hints and comprehensive documentation
  - Modular, maintainable codebase

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/noah-ing/theres-waldo.git
cd theres-waldo

# Create a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install the package
pip install -e ".[dev]"
```

## ğŸ¯ Quick Start

Find Waldo in an image:

```bash
find-waldo path/to/image.jpg
```

Or use the Python API:

```python
from waldo_finder import WaldoFinder

# Initialize finder with trained model
finder = WaldoFinder('models/best_model.pkl')

# Find Waldo!
results = finder.find_waldo('path/to/image.jpg')
```

## ğŸ‹ï¸ Training

Train your own model with custom configuration:

```bash
# Set up wandb for experiment tracking
wandb login

# Train with default configuration
python -m waldo_finder.train

# Train with custom configuration
python -m waldo_finder.train data.batch_size=32 training.learning_rate=0.0002
```

## ğŸ“Š Model Architecture

The model uses a Vision Transformer backbone with modern improvements:

- Pre-norm transformer blocks for stable training
- Learnable class token for global feature aggregation
- Sinusoidal positional embeddings
- DETR-style detection head for accurate localization

```
Input Image (640x640x3)
    â”‚
    â–¼
ViT Backbone (12 layers)
    â”‚
    â–¼
Detection Head
    â”‚
    â–¼
Bounding Box + Confidence
```

## ğŸ”§ Advanced Usage

### Custom Training Configuration

Modify `config/train.yaml` or override via command line:

```bash
python -m waldo_finder.train \
    model.num_heads=8 \
    model.dropout_rate=0.2 \
    training.batch_size=64
```

### Experiment Tracking

View training progress and compare experiments:

```bash
# Start training with wandb logging
python -m waldo_finder.train wandb.project=my-project wandb.name=exp-001

# View results at: https://wandb.ai/username/my-project
```

## ğŸ“ˆ Performance

The model achieves state-of-the-art performance on Where's Waldo puzzles:

- **Accuracy**: 95%+ detection rate on test set
- **Speed**: ~100ms inference time on modern GPU
- **Robustness**: Handles various image sizes and styles

## ğŸ¤ Contributing

Contributions are welcome! Please check out our [contribution guidelines](CONTRIBUTING.md).

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Original dataset from [Hey-Waldo](https://github.com/vc1492a/Hey-Waldo)
- Inspired by advances in vision transformers and object detection
- Built with JAX ecosystem tools (Flax, Optax)

## ğŸ“š Citation

If you use this code in your research, please cite:

```bibtex
@software{waldo_finder_2025,
  author = {Noah},
  title = {There's Waldo: Modern Vision Transformer for Finding Waldo},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/noah-ing/theres-waldo}
}
