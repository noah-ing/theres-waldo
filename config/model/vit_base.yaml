# Vision Transformer base configuration
model:
  # Architecture parameters - lighter for faster training
  num_heads: 8
  num_layers: 8
  hidden_dim: 512
  mlp_dim: 2048
  dropout_rate: 0.2  # Increased for better regularization
  
  # Training specific parameters
  weight_decay: 0.05  # Increased for better regularization
  warmup_steps: 1000
  max_grad_norm: 1.0
  
  # Regularization - increased for better generalization
  label_smoothing: 0.2
  mixup_alpha: 0.4
  
  # Optimization
  optimizer:
    type: adamw
    beta1: 0.9
    beta2: 0.999
    epsilon: 1.0e-8
    
  # Learning rate schedule
  lr_schedule:
    warmup_steps: 1000
    decay_steps: 50000
    min_lr_ratio: 0.01
    
    # Data augmentation - stronger for better generalization
  augmentation:
    random_flip: true
    color_jitter:
      brightness: 0.4
      contrast: 0.4
      saturation: 0.4
      hue: 0.2
